#!/usr/bin/env python3
"""
Complete LGS Exam Bundle Generator
Creates full LGS exams with exact subject distribution mixing original LGS + MILK questions
"""

import json
import random
import sys
from typing import Dict, List, Any, Tuple
from collections import defaultdict, Counter
from anthropic import Anthropic
import os

class CompleteLGSExamGenerator:
    def __init__(self):
        self.client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
        
        # Load all available data
        self.kazanimlar = self.load_kazanimlar()
        self.original_questions = self.load_original_questions()
        self.lgs_distribution = self.load_lgs_distribution()
        
        # Group kazanÄ±mlar by subject
        self.kazanim_by_subject = defaultdict(list)
        for kazanim in self.kazanimlar:
            subject = self.normalize_subject(kazanim.get('subject', ''))
            self.kazanim_by_subject[subject].append(kazanim)
        
        # Target LGS distribution
        self.target_distribution = {
            'TÃ¼rkÃ§e': 20,
            'Matematik': 20, 
            'Din KÃ¼ltÃ¼rÃ¼ ve Ahlak Bilgisi': 10,
            'Fen Bilimleri': 20,
            'Sosyal Bilgiler': 10,  # Ä°nkÄ±lap Tarihi + Geography
            'Ä°ngilizce': 10
        }
        
    def load_kazanimlar(self) -> List[Dict]:
        """Load all kazanÄ±mlar from multiple sources"""
        all_kazanimlar = []
        
        # Try multiple kazanim files
        kazanim_files = [
            '/app/extracted_kazanimlar_8_fixed.jsonl',
            '/app/enhanced_kazanimlar_complete.jsonl'
        ]
        
        for file_path in kazanim_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            kazanim = json.loads(line.strip())
                            all_kazanimlar.append(kazanim)
                print(f"âœ“ Loaded kazanÄ±mlar from {file_path}")
                break
            except:
                continue
        
        return all_kazanimlar
    
    def load_original_questions(self) -> List[Dict]:
        """Load original LGS questions"""
        questions = []
        try:
            with open('/app/enhanced_questions_final.jsonl', 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        question = json.loads(line.strip())
                        question['stamp'] = 'LGS'
                        question['source'] = 'Original LGS'
                        questions.append(question)
            print(f"âœ“ Loaded {len(questions)} original LGS questions")
        except:
            print("âš ï¸ Could not load original questions")
        
        return questions
    
    def load_lgs_distribution(self) -> Dict:
        """Load LGS historical distribution data"""
        try:
            with open('/app/lgs_distribution_analysis.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
            print("âœ“ Loaded LGS distribution patterns")
            return data
        except:
            print("âš ï¸ Could not load LGS distribution")
            return {}
    
    def normalize_subject(self, subject: str) -> str:
        """Normalize subject names to LGS standard"""
        subject = subject.strip()
        
        mappings = {
            'matematik': 'Matematik',
            'tÃ¼rkÃ§e': 'TÃ¼rkÃ§e', 
            'fen bilimleri': 'Fen Bilimleri',
            'din kÃ¼ltÃ¼rÃ¼ ve ahlak bilgisi': 'Din KÃ¼ltÃ¼rÃ¼ ve Ahlak Bilgisi',
            'din kÃ¼ltÃ¼rÃ¼': 'Din KÃ¼ltÃ¼rÃ¼ ve Ahlak Bilgisi',
            'Ä°nkÄ±lap tarihi ve atatÃ¼rkÃ§Ã¼lÃ¼k': 'Sosyal Bilgiler',
            'inkÄ±lap tarihi': 'Sosyal Bilgiler', 
            'tarih': 'Sosyal Bilgiler',
            'sosyal bilgiler': 'Sosyal Bilgiler',
            'english': 'Ä°ngilizce',
            'ingilizce': 'Ä°ngilizce'
        }
        
        subject_lower = subject.lower()
        for key, value in mappings.items():
            if key in subject_lower:
                return value
        
        return subject
    
    def get_topic_frequency(self, subject: str, kazanim_code: str) -> float:
        """Get topic frequency from LGS distribution analysis"""
        try:
            lgs_subjects = self.lgs_distribution.get('subjects', {})
            subject_data = lgs_subjects.get(subject, {})
            topics = subject_data.get('topics', [])
            
            for topic in topics:
                codes = topic.get('kazanim_codes', [])
                if any(code in kazanim_code for code in codes):
                    years_data = topic.get('years_data', {})
                    if years_data:
                        total_pct = sum(year_data.get('percentage', 0) for year_data in years_data.values())
                        return total_pct / len(years_data)
            
            return 5.0  # Default medium frequency
        except:
            return 5.0
    
    def determine_difficulty(self, kazanim: Dict, frequency: float) -> str:
        """Determine question difficulty based on kazanim and frequency"""
        description = kazanim.get('description', '').lower()
        
        # High frequency topics tend to be more accessible
        if frequency >= 15:  # High frequency
            if any(word in description for word in ['deÄŸerlendirir', 'analiz', 'sentez']):
                return 'Orta'
            else:
                return 'Kolay'
        elif frequency >= 5:  # Medium frequency  
            if any(word in description for word in ['deÄŸerlendirir', 'yaratÄ±r', 'tasarlar']):
                return 'Zor'
            elif any(word in description for word in ['analiz', 'karÅŸÄ±laÅŸtÄ±r', 'aÃ§Ä±klar']):
                return 'Orta'
            else:
                return 'Kolay'
        else:  # Low frequency
            return random.choice(['Orta', 'Zor'])
    
    def extract_json_from_response(self, response_text: str) -> Dict:
        """Extract and clean JSON from LLM response with robust error handling"""
        
        # Try different extraction methods
        extraction_methods = [
            # Method 1: Extract from code blocks
            lambda text: self._extract_from_code_block(text),
            # Method 2: Find JSON object boundaries
            lambda text: self._extract_json_boundaries(text),
            # Method 3: Line-by-line parsing
            lambda text: self._extract_line_by_line(text),
            # Method 4: Regex-based extraction
            lambda text: self._extract_with_regex(text)
        ]
        
        for i, method in enumerate(extraction_methods):
            try:
                json_text = method(response_text)
                if json_text:
                    return json.loads(json_text)
            except Exception as e:
                continue
        
        raise ValueError("Could not extract valid JSON from response")
    
    def _extract_from_code_block(self, text: str) -> str:
        """Extract JSON from ```json blocks"""
        if '```json' in text:
            start = text.find('```json') + 7
            end = text.find('```', start)
            if end > start:
                return text[start:end].strip()
        return ""
    
    def _extract_json_boundaries(self, text: str) -> str:
        """Find JSON object by matching { } boundaries"""
        lines = text.split('\n')
        json_lines = []
        brace_count = 0
        started = False
        
        for line in lines:
            if '{' in line and not started:
                started = True
                brace_count += line.count('{') - line.count('}')
                json_lines.append(line)
            elif started:
                brace_count += line.count('{') - line.count('}')
                json_lines.append(line)
                if brace_count <= 0:
                    break
        
        return '\n'.join(json_lines)
    
    def _extract_line_by_line(self, text: str) -> str:
        """Extract JSON by stopping at first complete object"""
        lines = text.split('\n')
        json_lines = []
        
        for line in lines:
            json_lines.append(line)
            potential_json = '\n'.join(json_lines)
            
            # Try to parse at each line to find complete JSON
            try:
                json.loads(potential_json)
                return potential_json
            except:
                continue
        
        return ""
    
    def _extract_with_regex(self, text: str) -> str:
        """Use regex to find JSON-like structures"""
        import re
        
        # Look for JSON object pattern
        pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
        matches = re.findall(pattern, text, re.DOTALL)
        
        for match in matches:
            try:
                json.loads(match)
                return match
            except:
                continue
        
        return ""

    def generate_milk_question(self, kazanim: Dict, subject: str, difficulty: str) -> Dict:
        """Generate MILK question with enhanced JSON parsing and special character support"""
        
        code = kazanim.get('code', 'Unknown')
        description = kazanim.get('description', 'No description')
        
        # Subject-specific examples
        subject_examples = {
            'Matematik': 'Ã–rnek: âˆš16 = 4, 2Â³ = 8, x â‰¥ 5, aÂ² + bÂ² = cÂ²',
            'Fen Bilimleri': 'Ã–rnek: Hâ‚‚O, COâ‚‚, DNA, RNA, oksijen, karbon',
            'TÃ¼rkÃ§e': 'Ã–rnek: anlam, sÃ¶zcÃ¼k, cÃ¼mle, paragraf, metin',
            'Din KÃ¼ltÃ¼rÃ¼ ve Ahlak Bilgisi': 'Ã–rnek: namaz, zekÃ¢t, hadis, ayet',
            'Sosyal Bilgiler': 'Ã–rnek: tarih, coÄŸrafya, AtatÃ¼rk, inkÄ±lap',
            'Ä°ngilizce': 'Ã–rnek: grammar, vocabulary, reading, listening'
        }
        
        examples = subject_examples.get(subject, '')
        
        prompt = f"""Sen LGS {subject} soru uzmanÄ±sÄ±n. Bu kazanÄ±m iÃ§in {difficulty} seviyesinde soru oluÅŸtur.

KAZANIM:
- Kod: {code}
- AÃ§Ä±klama: {description}

{examples}

Ã–NEMLÄ°: Sadece JSON formatÄ±nda yanÄ±t ver. HiÃ§bir ek aÃ§Ä±klama ekleme.

JSON FORMATINDA YANIT:
{{
  "stem": "Soru metni burada",
  "options": [
    {{"key": "A", "text": "SeÃ§enek A"}},
    {{"key": "B", "text": "SeÃ§enek B"}}, 
    {{"key": "C", "text": "SeÃ§enek C"}},
    {{"key": "D", "text": "SeÃ§enek D"}}
  ],
  "correct_answer": "B",
  "explanation": "Ã‡Ã¶zÃ¼m aÃ§Ä±klamasÄ±",
  "confidence": 90
}}"""

        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.client.messages.create(
                    model="claude-3-haiku-20240307",
                    max_tokens=1500,
                    temperature=0.5,
                    messages=[{"role": "user", "content": prompt}]
                )
                
                response_text = response.content[0].text.strip()
                
                # Use enhanced JSON extraction
                question_data = self.extract_json_from_response(response_text)
                
                # Validate required fields
                required_fields = ['stem', 'options', 'correct_answer', 'explanation']
                if all(field in question_data for field in required_fields):
                    
                    # Add metadata
                    question_data.update({
                        'kazanim_code': code,
                        'subject': subject,
                        'difficulty_level': difficulty,
                        'stamp': 'MILK',
                        'source': 'MILK Generated',
                        'generated_by': 'MILK-AI'
                    })
                    
                    return question_data
                else:
                    raise ValueError(f"Missing required fields: {required_fields}")
                    
            except Exception as e:
                if attempt == max_retries - 1:
                    print(f"âš ï¸ Error generating MILK question for {code} after {max_retries} attempts: {e}")
                    # Return fallback question
                    return {
                        'stem': f"{subject} konusunda bir soru: {description[:100]}...",
                        'options': [
                            {'key': 'A', 'text': 'SeÃ§enek A'},
                            {'key': 'B', 'text': 'SeÃ§enek B'},
                            {'key': 'C', 'text': 'SeÃ§enek C'}, 
                            {'key': 'D', 'text': 'SeÃ§enek D'}
                        ],
                        'correct_answer': 'A',
                        'explanation': 'Bu bir Ã¶rnek aÃ§Ä±klamadÄ±r.',
                        'confidence': 50,
                        'kazanim_code': code,
                        'subject': subject,
                        'difficulty_level': difficulty,
                        'stamp': 'MILK',
                        'source': 'MILK Fallback'
                    }
                else:
                    print(f"   âš ï¸ Attempt {attempt + 1}/{max_retries} failed for {code}: {e}")
                    continue
    
    def generate_questions_for_subject(self, subject: str, target_count: int, mix_ratio: float = 0.4) -> List[Dict]:
        """Generate mixed original + MILK questions for a subject"""
        
        print(f"  ğŸ“š {subject}: {target_count} questions (mix ratio: {int(mix_ratio*100)}% original)")
        
        # Calculate split
        original_target = int(target_count * mix_ratio) 
        milk_target = target_count - original_target
        
        questions = []
        
        # Get original questions for this subject
        original_available = [q for q in self.original_questions 
                            if self.normalize_subject(q.get('subject', '')) == subject]
        
        # Add original questions
        original_selected = random.sample(original_available, 
                                        min(original_target, len(original_available)))
        questions.extend(original_selected)
        
        actual_original = len(original_selected)
        remaining_milk = target_count - actual_original
        
        print(f"    ğŸ“„ Original: {actual_original}, ğŸ¥› MILK needed: {remaining_milk}")
        
        # Generate MILK questions
        available_kazanimlar = self.kazanim_by_subject.get(subject, [])
        
        if not available_kazanimlar:
            print(f"    âš ï¸ No kazanÄ±mlar found for {subject}")
            return questions
        
        difficulties = ['Kolay', 'Orta', 'Zor']
        difficulty_weights = [0.4, 0.4, 0.2]  # 40% easy, 40% medium, 20% hard
        
        for i in range(remaining_milk):
            # Select random kazanim
            kazanim = random.choice(available_kazanimlar)
            
            # Select weighted difficulty
            difficulty = random.choices(difficulties, weights=difficulty_weights)[0]
            
            # Get frequency
            frequency = self.get_topic_frequency(subject, kazanim.get('code', ''))
            
            print(f"    â³ MILK {i+1}/{remaining_milk}: {kazanim.get('code', 'No code')} ({difficulty})")
            
            # Generate question
            milk_question = self.generate_milk_question(kazanim, subject, difficulty)
            
            if milk_question:
                questions.append(milk_question)
                confidence = milk_question.get('confidence', 0)
                print(f"       âœ“ Generated ({confidence}% confidence)")
            else:
                print(f"       âœ— Failed")
        
        return questions
    
    def create_complete_lgs_bundle(self, bundle_name: str = "Complete LGS Bundle") -> Dict:
        """Create complete LGS exam bundle with all subjects"""
        
        print(f"ğŸ¯ GENERATING {bundle_name.upper()}")
        print("=" * 60)
        
        all_questions = []
        bundle_stats = {
            'by_subject': {},
            'by_difficulty': defaultdict(int),
            'by_source': defaultdict(int),
            'total_questions': 0
        }
        
        print(f"\nğŸ“Š Target Distribution:")
        for subject, count in self.target_distribution.items():
            print(f"   {subject}: {count} questions")
        
        print(f"\nğŸ”„ Generating questions...")
        
        # Generate questions for each subject
        for subject, target_count in self.target_distribution.items():
            subject_questions = self.generate_questions_for_subject(subject, target_count)
            
            # Update statistics
            bundle_stats['by_subject'][subject] = len(subject_questions)
            bundle_stats['total_questions'] += len(subject_questions)
            
            for question in subject_questions:
                source = question.get('stamp', 'Unknown')
                difficulty = question.get('difficulty_level', 'Unknown')
                
                bundle_stats['by_source'][source] += 1
                bundle_stats['by_difficulty'][difficulty] += 1
            
            all_questions.extend(subject_questions)
        
        # Add question numbers and shuffle
        for i, question in enumerate(all_questions, 1):
            question['question_number'] = i
        
        random.shuffle(all_questions)
        
        # Re-number after shuffle
        for i, question in enumerate(all_questions, 1):
            question['question_number'] = i
        
        return {
            'bundle_info': {
                'title': bundle_name,
                'description': 'Complete LGS exam with all subjects mixed original + MILK questions',
                'total_questions': len(all_questions),
                'subjects': list(self.target_distribution.keys()),
                'distribution_type': 'Standard LGS',
                'generation_date': '2024-11-14',
                'version': '1.0'
            },
            'target_distribution': self.target_distribution,
            'questions': all_questions,
            'statistics': {
                'by_subject': dict(bundle_stats['by_subject']),
                'by_difficulty': dict(bundle_stats['by_difficulty']),
                'by_source': dict(bundle_stats['by_source']),
                'total_questions': bundle_stats['total_questions']
            }
        }

def main():
    print("ğŸ¯ COMPLETE LGS EXAM BUNDLE GENERATOR")
    print("=" * 60)
    
    generator = CompleteLGSExamGenerator()
    
    print(f"âœ“ Loaded {len(generator.kazanimlar)} kazanÄ±mlar")
    print(f"âœ“ Loaded {len(generator.original_questions)} original questions")
    print(f"âœ“ Found subjects: {list(generator.kazanim_by_subject.keys())}")
    
    # Generate bundle
    bundle = generator.create_complete_lgs_bundle("LGS Karma Deneme - Tam Set")
    
    # Save bundle
    output_file = '/app/complete_lgs_exam_bundle.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(bundle, f, ensure_ascii=False, indent=2)
    
    # Display results
    stats = bundle['statistics']
    
    print(f"\n" + "=" * 60)
    print(f"ğŸ“Š COMPLETE LGS BUNDLE SUMMARY")
    print("=" * 60)
    print(f"Total questions: {stats['total_questions']}")
    
    print(f"\nğŸ“š By Subject:")
    target = bundle['target_distribution']
    for subject, actual in stats['by_subject'].items():
        target_count = target.get(subject, 0)
        percentage = (actual / target_count) * 100 if target_count > 0 else 0
        print(f"   {subject}: {actual}/{target_count} ({percentage:.1f}%)")
    
    print(f"\nğŸ·ï¸ By Source:")
    for source, count in stats['by_source'].items():
        percentage = (count / stats['total_questions']) * 100
        print(f"   {source}: {count} ({percentage:.1f}%)")
    
    print(f"\nğŸ¯ By Difficulty:")
    for difficulty, count in stats['by_difficulty'].items():
        percentage = (count / stats['total_questions']) * 100
        print(f"   {difficulty}: {count} ({percentage:.1f}%)")
    
    print(f"\nâœ… Bundle saved to: {output_file}")
    print(f"ğŸš€ Complete LGS exam bundle ready for deployment!")

if __name__ == "__main__":
    main()