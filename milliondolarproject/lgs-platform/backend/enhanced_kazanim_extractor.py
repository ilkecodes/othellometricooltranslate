#!/usr/bin/env python3
"""
Enhanced KazanÄ±m Extractor with Special Character Support
Re-extracts kazanÄ±mlar with better Unicode and special character handling
"""

import sys
import json
import pdfplumber
import re
from pathlib import Path
from typing import Dict, List, Any
from anthropic import Anthropic
import os

def extract_text_with_special_chars(pdf_path: str) -> Dict[int, str]:
    """Extract text from PDF with better Unicode handling"""
    pages_text = {}
    
    with pdfplumber.open(pdf_path) as pdf:
        print(f"ğŸ“„ Re-processing PDF with special char support: {pdf_path}")
        print(f"ğŸ“Š Total pages: {len(pdf.pages)}")
        
        for page_num, page in enumerate(pdf.pages, 1):
            try:
                # Extract with different methods to catch special chars
                text = page.extract_text()
                
                # Try to extract tables as well for structured data
                tables = page.extract_tables()
                if tables:
                    for table in tables:
                        for row in table:
                            if row:
                                text += "\n" + " ".join([cell for cell in row if cell])
                
                if text and len(text.strip()) > 50:
                    # Clean and normalize Unicode
                    text = text.strip()
                    # Preserve mathematical symbols
                    text = text.replace('', 'âˆš')  # Square root if corrupted
                    text = text.replace('Â²', 'Â²')  # Superscript 2
                    text = text.replace('Â³', 'Â³')  # Superscript 3
                    text = text.replace('â‰¥', 'â‰¥')  # Greater than or equal
                    text = text.replace('â‰¤', 'â‰¤')  # Less than or equal
                    text = text.replace('Â±', 'Â±')  # Plus minus
                    
                    pages_text[page_num] = text
                    print(f"âœ“ Page {page_num}: {len(text)} characters (special chars preserved)")
                else:
                    print(f"âŠ˜ Page {page_num}: Too short, skipping")
            except Exception as e:
                print(f"âœ— Error on page {page_num}: {e}")
                
    return pages_text

def enhanced_kazanim_extraction(page_text: str, page_num: int) -> Dict[str, Any]:
    """Enhanced kazanÄ±m extraction with better subject detection"""
    
    client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
    
    prompt = f"""Bu sayfa 8. sÄ±nÄ±f kazanÄ±mlarÄ± iÃ§eriyor. BÃœTÃœN KONULARI ve kazanÄ±mlarÄ± dikkatli ÅŸekilde Ã§Ä±kar.

Ã–ZEL DÄ°KKAT:
1. TÃœM LGS KONULARINI ara: TÃ¼rkÃ§e, Matematik, Fen Bilimleri, Sosyal Bilgiler, Ä°nkÄ±lap Tarihi, Din KÃ¼ltÃ¼rÃ¼, Ä°ngilizce
2. Ã–zel karakterleri koru: âˆš, Â², Â³, Â±, â‰¥, â‰¤ gibi matematik sembolleri
3. Her kazanÄ±mÄ±n tam kodunu Ã§Ä±kar (M.8.1.1.1 formatÄ±nda)
4. Konu baÅŸlÄ±klarÄ±nÄ± ve alt baÅŸlÄ±klarÄ± yakala

Sayfa Ä°Ã§eriÄŸi:
{page_text}

JSON formatÄ±nda Ã§Ä±kar:
{{
  "page_number": {page_num},
  "subjects_found": ["Matematik", "Fen Bilimleri", "vb..."],
  "kazanimlar": [
    {{
      "subject": "Matematik",
      "unit": "SayÄ±lar ve Ä°ÅŸlemler", 
      "topic": "Ã‡arpanlar ve Katlar",
      "code": "M.8.1.1.1",
      "description": "Verilen pozitif tam sayÄ±larÄ±n pozitif tam sayÄ± Ã§arpanlarÄ±nÄ± bulur, pozitif tam sayÄ±larÄ±n pozitif tam sayÄ± Ã§arpanlarÄ±nÄ± Ã¼slÃ¼ ifadelerin Ã§arpÄ±mÄ± ÅŸeklinde yazar.",
      "grade": "8",
      "notes": "Bir pozitif tam sayÄ±nÄ±n asal Ã§arpanlarÄ±nÄ± bulmaya yÃ¶nelik Ã§alÄ±ÅŸmalara da yer verilir."
    }}
  ],
  "confidence": 90
}}

MUTLAKA TÃœM KONULARI VE Ã–ZEL KARAKTERLERÄ° Ã‡IKAR!"""

    try:
        response = client.messages.create(
            model="claude-3-haiku-20240307",
            max_tokens=4000,
            messages=[{"role": "user", "content": prompt}]
        )
        
        response_text = response.content[0].text.strip()
        
        # Clean up response
        if response_text.startswith('```json'):
            response_text = response_text[7:]
        if response_text.endswith('```'):
            response_text = response_text[:-3]
        
        return json.loads(response_text)
        
    except json.JSONDecodeError as e:
        print(f"âš ï¸ JSON parsing failed on page {page_num}: {e}")
        return {"page_number": page_num, "confidence": 0, "error": str(e)}
    except Exception as e:
        print(f"âœ— LLM analysis failed on page {page_num}: {e}")
        return {"page_number": page_num, "confidence": 0, "error": str(e)}

def main():
    if len(sys.argv) != 3:
        print("Usage: python enhanced_kazanim_extractor.py <input_pdf> <output_jsonl>")
        sys.exit(1)
    
    pdf_path = sys.argv[1]
    output_path = sys.argv[2]
    
    if not Path(pdf_path).exists():
        print(f"âœ— Error: PDF file not found: {pdf_path}")
        sys.exit(1)
    
    print("ğŸ“š ENHANCED KAZANIM EXTRACTION WITH SPECIAL CHARACTERS")
    print("=" * 70)
    
    # Step 1: Extract text with special character support
    print("\n1ï¸âƒ£ Extracting text with Unicode support...")
    pages_text = extract_text_with_special_chars(pdf_path)
    
    if not pages_text:
        print("âœ— No readable content found in PDF")
        sys.exit(1)
    
    print(f"âœ“ Extracted text from {len(pages_text)} pages with special characters")
    
    # Step 2: Enhanced kazanÄ±m extraction
    print("\n2ï¸âƒ£ Enhanced kazanÄ±m extraction...")
    all_kazanimlar = []
    all_subjects = set()
    successful_pages = 0
    total_confidence = 0
    
    for page_num, text in pages_text.items():
        print(f"   â³ Processing page {page_num}...")
        
        extraction = enhanced_kazanim_extraction(text, page_num)
        confidence = extraction.get('confidence', 0)
        
        if confidence > 50:
            print(f"   âœ“ Page {page_num}: {confidence}% confidence")
            successful_pages += 1
            total_confidence += confidence
            
            # Extract kazanÄ±mlar
            kazanimlar = extraction.get('kazanimlar', [])
            all_kazanimlar.extend(kazanimlar)
            
            # Track subjects
            subjects_found = extraction.get('subjects_found', [])
            all_subjects.update(subjects_found)
            
            print(f"      ğŸ“š Found {len(kazanimlar)} kazanÄ±mlar, subjects: {subjects_found}")
        else:
            print(f"   âŠ˜ Page {page_num}: Low confidence ({confidence}%), skipping")
    
    # Step 3: Save enhanced results
    print("\n3ï¸âƒ£ Saving enhanced kazanÄ±mlar...")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for kazanim in all_kazanimlar:
            json.dump(kazanim, f, ensure_ascii=False)
            f.write('\n')
    
    avg_confidence = total_confidence / successful_pages if successful_pages > 0 else 0
    
    # Display summary
    print("\n" + "=" * 70)
    print("ğŸ“Š ENHANCED EXTRACTION SUMMARY")
    print("=" * 70)
    print(f"Source PDF: {pdf_path}")
    print(f"Pages processed: {len(pages_text)}")
    print(f"Successful extractions: {successful_pages}")
    print(f"Average confidence: {avg_confidence:.1f}%")
    print(f"Total kazanÄ±mlar extracted: {len(all_kazanimlar)}")
    print(f"Subjects found: {len(all_subjects)}")
    
    # Subject breakdown
    subject_counts = {}
    for kazanim in all_kazanimlar:
        subject = kazanim.get('subject', 'Unknown')
        subject_counts[subject] = subject_counts.get(subject, 0) + 1
    
    print(f"\nğŸ“š Subject Distribution:")
    for subject, count in sorted(subject_counts.items()):
        print(f"   {subject}: {count} kazanÄ±mlar")
    
    # Check for LGS subjects
    lgs_subjects = {'TÃ¼rkÃ§e', 'Matematik', 'Fen Bilimleri', 'Sosyal Bilgiler', 'Ä°nkÄ±lap Tarihi', 'Din KÃ¼ltÃ¼rÃ¼', 'Ä°ngilizce'}
    found_lgs = set(all_subjects) & lgs_subjects
    missing_lgs = lgs_subjects - set(all_subjects)
    
    print(f"\nğŸ¯ LGS Subject Coverage:")
    print(f"   Found: {sorted(found_lgs)}")
    if missing_lgs:
        print(f"   Missing: {sorted(missing_lgs)}")
    else:
        print(f"   âœ… All LGS subjects covered!")
    
    print(f"\nOutput file: {output_path}")
    print("âœ… Enhanced extraction completed successfully!")

if __name__ == "__main__":
    main()